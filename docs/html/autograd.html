
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>autograd package &#8212; ToeffiPy 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="autograd.data package" href="autograd.data.html" />
    <link rel="prev" title="ToeffiPy" href="modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="autograd-package">
<h1>autograd package<a class="headerlink" href="#autograd-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="autograd.data.html">autograd.data package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.data.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.data.html#module-autograd.data.dataloader">autograd.data.dataloader module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.data.html#module-autograd.data.dataset">autograd.data.dataset module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.data.html#module-autograd.data">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.nn.html">autograd.nn package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.activation">autograd.nn.activation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.functional">autograd.nn.functional module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.layer">autograd.nn.layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.lossfunction">autograd.nn.lossfunction module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.module">autograd.nn.module module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.optim">autograd.nn.optim module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn.parameter">autograd.nn.parameter module</a></li>
<li class="toctree-l2"><a class="reference internal" href="autograd.nn.html#module-autograd.nn">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-autograd.function">
<span id="autograd-function-module"></span><h2>autograd.function module<a class="headerlink" href="#module-autograd.function" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="autograd.function.cos">
<code class="descclassname">autograd.function.</code><code class="descname">cos</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the cos function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.elu">
<code class="descclassname">autograd.function.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em>, <em>alpha: float = 1.0</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the elu function in autograd
:param tensor: (Tensor) Input tensor
:param alpha: (float) Alpha parameter of exponential slope
:return: (Tensor) Output Tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.exp">
<code class="descclassname">autograd.function.</code><code class="descname">exp</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the element-wise exponential function
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.leaky_relu">
<code class="descclassname">autograd.function.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em>, <em>negative_slope: float = 0.2</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the leaky-relu function in autograd
:param tensor: (Tensor) Input tensor
:param negative_slope: (float) Negative slope of leaky-relu
:return: (Tensor) Output Tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.log">
<code class="descclassname">autograd.function.</code><code class="descname">log</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the natural logarithm in autograd.
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.relu">
<code class="descclassname">autograd.function.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the relu function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output Tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.selu">
<code class="descclassname">autograd.function.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the selu function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output Tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.sigmoid">
<code class="descclassname">autograd.function.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the sigmoid function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.sin">
<code class="descclassname">autograd.function.</code><code class="descname">sin</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the sin function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.softplus">
<code class="descclassname">autograd.function.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the softplus function in autograd.
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.sqrt">
<code class="descclassname">autograd.function.</code><code class="descname">sqrt</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the square root in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.tan">
<code class="descclassname">autograd.function.</code><code class="descname">tan</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the tan function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.function.tanh">
<code class="descclassname">autograd.function.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.function.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the tanh function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

</div>
<div class="section" id="module-autograd.tensor">
<span id="autograd-tensor-module"></span><h2>autograd.tensor module<a class="headerlink" href="#module-autograd.tensor" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="autograd.tensor.Dependency">
<em class="property">class </em><code class="descclassname">autograd.tensor.</code><code class="descname">Dependency</code><span class="sig-paren">(</span><em>activation: Tensor, grad_fn: Callable[[np.ndarray], np.ndarray]</em><span class="sig-paren">)</span><a class="headerlink" href="#autograd.tensor.Dependency" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Data class including activation and gradient function</p>
</dd></dl>

<dl class="class">
<dt id="autograd.tensor.Tensor">
<em class="property">class </em><code class="descclassname">autograd.tensor.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>data: Union[int, list, float, numpy.ndarray, autograd.tensor.Tensor], requires_grad: bool = False, dependencies: Optional[List[T]] = None</em><span class="sig-paren">)</span><a class="headerlink" href="#autograd.tensor.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class implements an autograd tensor</p>
<dl class="method">
<dt id="autograd.tensor.Tensor.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>grad: Optional[autograd.tensor.Tensor] = None</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#autograd.tensor.Tensor.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Method computes the gradient of the tensor
:param grad: (Tensor) Previous gradient</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Method clones a given tensor but input and output tensors remain in graph
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="attribute">
<dt id="autograd.tensor.Tensor.data">
<code class="descname">data</code><a class="headerlink" href="#autograd.tensor.Tensor.data" title="Permalink to this definition">¶</a></dt>
<dd><p>Getter method for _data
:return: (np.ndarray) Data of tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.max">
<code class="descname">max</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#autograd.tensor.Tensor.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Method returns the max value of the tensor
:return: (float) Tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.mean">
<code class="descname">mean</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Method computes the mean of a tensor.
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.squeeze">
<code class="descname">squeeze</code><span class="sig-paren">(</span><em>dim: int = -1</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Method removes a dimension to the tensor at a given position.
:param dim: (int) Position to add the dimension
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.std">
<code class="descname">std</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.std" title="Permalink to this definition">¶</a></dt>
<dd><p>Method computes the standard deviation of a tensor.
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.sum">
<code class="descname">sum</code><span class="sig-paren">(</span><em>axis: Optional[int] = None</em>, <em>keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Method sums up a given tensor.
:param axis: (int) Axis to apply summation
:param keepdims: (bool) If true summed up dimensions are retained
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.unsqueeze">
<code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>dim: int = -1</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Method adds a dimension to the tensor into the given position.
:param dim: (int) Position to add the dimension
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.var">
<code class="descname">var</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.Tensor.var" title="Permalink to this definition">¶</a></dt>
<dd><p>Method computes the variance of a tensor.
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="method">
<dt id="autograd.tensor.Tensor.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#autograd.tensor.Tensor.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Method sets gradients to zero</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="autograd.tensor.abs">
<code class="descclassname">autograd.tensor.</code><code class="descname">abs</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the absolute function in autograd.
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.add">
<code class="descclassname">autograd.tensor.</code><code class="descname">add</code><span class="sig-paren">(</span><em>tensor_1: autograd.tensor.Tensor</em>, <em>tensor_2: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements a differentiable addition of two tensors. Broadcasting is supported.
:param tensor_1: (Tensor) First tensor
:param tensor_2: (Tensor) Second tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.clone">
<code class="descclassname">autograd.tensor.</code><code class="descname">clone</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Function makes a copy of a given tensor but input tensor remains in graph
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.div">
<code class="descclassname">autograd.tensor.</code><code class="descname">div</code><span class="sig-paren">(</span><em>tensor_1: autograd.tensor.Tensor</em>, <em>tensor_2: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.div" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements a differentiable elemnt wise division of two tensors. Broadcasting is supported.
:param tensor_1: (Tensor) First tensor
:param tensor_2: (Tensor) Second tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.flatten">
<code class="descclassname">autograd.tensor.</code><code class="descname">flatten</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em>, <em>starting_dim: int = 1</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>This function implements a flatten operation which reshapes (flattens) a given tensor
:param tensor: (Tensor) Input tensor
:param starting_dim: (int) Dimension to start flattening
:return: (Tensor) Flattened output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.matmul">
<code class="descclassname">autograd.tensor.</code><code class="descname">matmul</code><span class="sig-paren">(</span><em>tensor_1: autograd.tensor.Tensor</em>, <em>tensor_2: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements a differentiable matrix multiplication of two tensors.
:param tensor_1: (Tensor) First Tensor
:param tensor_2: (Tensor) Second Tensor
:return: (Tensor) Result tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.max">
<code class="descclassname">autograd.tensor.</code><code class="descname">max</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; float<a class="headerlink" href="#autograd.tensor.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of the max function for autograd tensors
:param tensor: (Tensor) Input tensor
:return: (float) Max value</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.mean">
<code class="descclassname">autograd.tensor.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the mean function in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output scalar tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.mul">
<code class="descclassname">autograd.tensor.</code><code class="descname">mul</code><span class="sig-paren">(</span><em>tensor_1: autograd.tensor.Tensor</em>, <em>tensor_2: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.mul" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements a differentiable multiplication of two tensors. Broadcasting is supported.
:param tensor_1: (Tensor) First tensor
:param tensor_2: (Tensor) Second tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.neg">
<code class="descclassname">autograd.tensor.</code><code class="descname">neg</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>Function negates a given tensor in autograd
:param tensor: (Tensor) Input tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.pad_1d">
<code class="descclassname">autograd.tensor.</code><code class="descname">pad_1d</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor, pad_width: Tuple[int, int], value: float = 0.0</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.pad_1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Function applies padding to a given 1d tensor (batch size, <a href="#id1"><span class="problematic" id="id2">*</span></a>, features), * various dimension
:param tensor: (Tensor) Input tensor of shape (batch size, <a href="#id3"><span class="problematic" id="id4">*</span></a>, features)
:param pad_width: (Tuple[int, int]) Number of values padded to the edges of each axis
:param value: (float) Padding value
:return: (Tensor) Output tensor (batch size, <a href="#id5"><span class="problematic" id="id6">*</span></a>, features + padded width)</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.pad_2d">
<code class="descclassname">autograd.tensor.</code><code class="descname">pad_2d</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor, pad_width: Tuple[Tuple[int, int], Tuple[int, int]], value: float = 0.0</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.pad_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Function applies padding to a given 2d tensor (batch size, channels, height, width)
:param tensor: (Tensor) Input tensor of shape (batch size, channels, height, width)
:param pad_width: (Tuple[Tuple[int, int], Tuple[int, int]]) Number of values padded to the edges of each axis
:param value: (float) Padding value
:return: (Tensor) Output tensor (batch size, channels, height + padded width, width + padded width)</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.pow">
<code class="descclassname">autograd.tensor.</code><code class="descname">pow</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor, power: Union[int, float]</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the element-wise power function in autograd.
:param tensor: (Tensor) Input tensor
:param power: (int, float) Exponent
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.squeeze">
<code class="descclassname">autograd.tensor.</code><code class="descname">squeeze</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em>, <em>dim: int = -1</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Function removes a dimension of a given tensor
:param tensor: (Tensor) Input tensor
:param dim: (int) Position where the new dimension is places
:return: (Tensor) Output tensor with increased dimensionality of one</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.stack">
<code class="descclassname">autograd.tensor.</code><code class="descname">stack</code><span class="sig-paren">(</span><em>tensors: List[autograd.tensor.Tensor], dim: int = 0</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Function stacks a list of tensor up to one tensor. Autograd is not supported!
:param tensors: (List[Tensor]) List of tensors
:param dim: (int) Dimension to stack
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.std">
<code class="descclassname">autograd.tensor.</code><code class="descname">std</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.std" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the standard deviation in autograd.
:param tensor: (Tensor) Input tensor
:return: (Tensor) Scalar output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.sub">
<code class="descclassname">autograd.tensor.</code><code class="descname">sub</code><span class="sig-paren">(</span><em>tensor_1: autograd.tensor.Tensor</em>, <em>tensor_2: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements a differentiable subtraction
:param tensor_1: (Tensor) First tensor
:param tensor_2: (Tensor) Second tensor
:return: (Tensor) Output tensor</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.sum">
<code class="descclassname">autograd.tensor.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em>, <em>axis: Optional[int] = None</em>, <em>keepdims: bool = False</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Sums up a tensor to a scalar value
:param tensor: (Tensor) Input tensor
:param axis: (int) Axis to apply summation
:param keepdims: (bool) If true summed up dimensions are retained
:return: (Tensor) Output tensor (scalar)</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.unsqueeze">
<code class="descclassname">autograd.tensor.</code><code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em>, <em>dim: int = -1</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Function adds a dimension to a given tensor
:param tensor: (Tensor) Input tensor
:param dim: (int) Position where the new dimension is places
:return: (Tensor) Output tensor with increased dimensionality of one</p>
</dd></dl>

<dl class="function">
<dt id="autograd.tensor.var">
<code class="descclassname">autograd.tensor.</code><code class="descname">var</code><span class="sig-paren">(</span><em>tensor: autograd.tensor.Tensor</em><span class="sig-paren">)</span> &#x2192; autograd.tensor.Tensor<a class="headerlink" href="#autograd.tensor.var" title="Permalink to this definition">¶</a></dt>
<dd><p>Function implements the variance function in autograd.
:param tensor: (Tensor) Input tensor
:return: (Tensor) Scalar output tensor</p>
</dd></dl>

</div>
<div class="section" id="module-autograd">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-autograd" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">ToeffiPy</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">ToeffiPy</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">autograd package</a></li>
<li class="toctree-l2"><a class="reference internal" href="setup.html">setup module</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="modules.html">ToeffiPy</a><ul>
      <li>Previous: <a href="modules.html" title="previous chapter">ToeffiPy</a></li>
      <li>Next: <a href="autograd.data.html" title="next chapter">autograd.data package</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Christoph Reich.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/autograd.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>